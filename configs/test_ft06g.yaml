# FT
#model:
#  target: backbone.face_transformer.FaceTransformerBackbone
#  params:
#    cnn_layers: [2, 2, 12, 1]
#    pattern_dim: 512
#    depth: 9
#    dim: 512
#    heads: 12
#    dim_head: 64
#    mlp_dim: 2048
#    emb_dropout: 0.1
#    dropout: 0.1
#    feature_dim: 512
#    use_cls_token: False
#    fp16: False

# FPT
#model:
#  target: backbone.face_transformer.FacePoolTransformerBackbone
#  params:
#    cnn_layers: [1, 1, 1]
#    pattern_dim: 256
#    depths: [12, 3]
#    dim: 256
#    heads: 16
#    dim_head: 64
#    mlp_dim: 1024
#    emb_dropout: 0.1
#    dropout: 0.1
#    feature_dim: 512
#    fp16: False

# FET
model:
  target: backbone.face_transformer.FaceEarlyTransformerBackbone
  params:
    start_channel: 48
    early_depths: 4
    depth: 12
    dim: 384
    heads: 8
    dim_head: 64
    mlp_dim: 1152
    emb_dropout: 0.1
    dropout: 0.1
    feature_dim: 512
    fp16: False
