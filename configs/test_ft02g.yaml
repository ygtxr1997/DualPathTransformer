## FT
#model:
#  target: backbone.face_transformer.FaceTransformerBackbone
#  params:
#    cnn_layers: [1, 1, 1, 1]
#    pattern_dim: 512
#    depth: 4
#    dim: 512
#    heads: 8
#    dim_head: 64
#    mlp_dim: 1536
#    emb_dropout: 0.1
#    dropout: 0.1
#    feature_dim: 512
#    use_cls_token: False
#    fp16: False

# FPT
#model:
#  target: backbone.face_transformer.FacePoolTransformerBackbone
#  params:
#    cnn_layers: [1, 1, 1]
#    pattern_dim: 256
#    depths: [2, 4]
#    dim: 256
#    heads: 8
#    dim_head: 64
#    mlp_dim: 1024
#    emb_dropout: 0.1
#    dropout: 0.1
#    feature_dim: 512
#    fp16: False

# FET
model:
  target: backbone.face_transformer.FaceEarlyTransformerBackbone
  params:
    start_channel: 24
    early_depths: 4
    depth: 8
    dim: 256
    heads: 6
    dim_head: 64
    mlp_dim: 768
    emb_dropout: 0.1
    dropout: 0.1
    feature_dim: 512
    fp16: False
